{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import apply_features\n",
    "from nltk import NaiveBayesClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    \n",
    "    def extract_tokens(self, text, target):\n",
    "        \"\"\"returns array of tuples where each tuple is defined by (tokenized_text, label)\n",
    "         parameters:\n",
    "                text: array of texts\n",
    "                target: array of target labels\n",
    "                \n",
    "        NOTE: consider only those words which have all alphabets and atleast 3 characters.\n",
    "        \"\"\"\n",
    "        l1 = []\n",
    "        for i in text:\n",
    "            t = [i1.lower() for i1 in i.split() if i1.isalpha() == True]\n",
    "            l1.extend(t)\n",
    "            \n",
    "            \n",
    "\n",
    "        corpus= [(i, j) for i, j in zip(l1, target) if (len(i) >= 3) ]\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    def get_features(self, corpus):\n",
    "        \"\"\" \n",
    "        returns a Set of unique words in complete corpus. \n",
    "        parameters:- corpus: tokenized corpus along with target labels (i.e)the ouput of extract_tokens function.\n",
    "        \n",
    "        Return Type is a set\n",
    "        \"\"\"\n",
    "#         print(set(corpus))\n",
    "        return set(corpus)        \n",
    "    \n",
    "    def extract_features(self, document):\n",
    "        \"\"\"\n",
    "        maps each input text into feature vector\n",
    "        parameters:- document: string\n",
    "        \n",
    "        Return type : A dictionary with keys being the train data set word features.\n",
    "                      The values correspond to True or False\n",
    "        \"\"\"\n",
    "        features={}\n",
    "        doc_words = set(document)\n",
    "        #iterate through the word_features to find if the doc_words contains it or not\n",
    "        for word in self.word_features:\n",
    "            features[word] = (word in doc_words)\n",
    "            \n",
    "        return features\n",
    "        \n",
    "\n",
    "    def train(self, text, labels):\n",
    "        \"\"\"\n",
    "        Returns trained model and set of unique words in training data\n",
    "        also set trained model to 'self.classifier' variable and set of \n",
    "        unique words to 'self.word_features' variable.\n",
    "        \"\"\"\n",
    "        #call extract_tokens\n",
    "        self.corpus= self.extract_tokens(text, labels)\n",
    "\n",
    "        #call get_features\n",
    "        self.word_features= self.get_features(self.corpus)\n",
    "\n",
    "        #Extracting training set\n",
    "        train_set = nltk.classify.util.apply_features(self.extract_features, self.corpus)\n",
    "    \n",
    "        #Now train the NaiveBayesClassifier with train_set\n",
    "        self.classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "        return self.classifier, self.word_features\n",
    "       \n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Returns prediction labels of given input text. \n",
    "        Allowed Text can be a simple string i.e one input email, a list of emails, or a dictionary of emails identified by their labels.\n",
    "        \"\"\"\n",
    "        if isinstance(text, (list)):\n",
    "            pred = []\n",
    "            for sentence in list(text):\n",
    "                pred.append(self.classifier.classify(self.extract_features(sentence.split())))\n",
    "            return pred\n",
    "        if isinstance(text, (collections.OrderedDict)):\n",
    "            pred = collections.OrderedDict()\n",
    "            for label, sentence in text.items():\n",
    "                pred[label] = self.classifier.classify(self.extract_features(sentence.split()))\n",
    "            return pred\n",
    "        return self.classifier.classify(self.extract_features(text.split()))\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data = pd.read_csv('email.csv')\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(data[\"text\"].values,\n",
    "                                                            data[\"spam\"].values,\n",
    "                                                            test_size = 0.25,\n",
    "                                                            random_state = 50,\n",
    "                                                            shuffle = True,\n",
    "                                                            stratify=data[\"spam\"].values)\n",
    "    classifier = SpamClassifier()\n",
    "    classifier_model, model_word_features = classifier.train(train_X, train_Y)\n",
    "    model_name = 'spam_classifier_model.pk'\n",
    "    model_word_features_name = 'spam_classifier_model_word_features.pk'\n",
    "    with open(model_name, 'wb') as model_fp:\n",
    "        pickle.dump(classifier_model, model_fp)\n",
    "    with open(model_word_features_name, 'wb') as model_fp:\n",
    "            pickle.dump(model_word_features, model_fp)\n",
    "    print('DONE')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: this free 7 - day trial will prove th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: followup from iris mack  hi ,  thank ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: make your rivals envy  lt is really h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: re : telephone interview with the enr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: a 1 time charge add your property / s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: this free 7 - day trial will prove th...     1\n",
       "1  Subject: followup from iris mack  hi ,  thank ...     0\n",
       "2  Subject: make your rivals envy  lt is really h...     1\n",
       "3  Subject: re : telephone interview with the enr...     0\n",
       "4  Subject: a 1 time charge add your property / s...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"email.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "        \"\"\"\n",
    "        maps each input text into feature vector\n",
    "        parameters:- document: string\n",
    "        \n",
    "        Return type : A dictionary with keys being the train data set word features.\n",
    "                      The values correspond to True or False\n",
    "        \"\"\"\n",
    "#         assert self.word_features\n",
    "        features={}\n",
    "        doc_words = set(document)\n",
    "        #iterate through the word_features to find if the doc_words contains it or not\n",
    "        for word in [\"a\", \"an\", \"the\", \"youuu\"]:\n",
    "            features['contains(%s)' % word] = (word in doc_words)\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contains(a)': True,\n",
       " 'contains(an)': True,\n",
       " 'contains(the)': True,\n",
       " 'contains(youuu)': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features([\"a\", \"an\", \"the\", \"you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " def extract_tokens(text, target):\n",
    "        \"\"\"returns array of tuples where each tuple is defined by (tokenized_text, label)\n",
    "         parameters:\n",
    "                text: array of texts\n",
    "                target: array of target labels\n",
    "                \n",
    "        NOTE: consider only those words which have all alphabets and atleast 3 characters.\n",
    "        \"\"\"\n",
    "        corpus= [(i.lower(), j) for i, j in zip(text, target) if (len(str(i)) >= 3 and i.isalpha() == True) ]\n",
    "      \n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abfkkjs', 1), ('the', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tokens([\"abfkkjs\", \"an\", \"The\", \"132131651\", \"12334566\"], [1, 0, 1, 0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c2b6dbf3fad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"dsnd llkdnlnsd klsdl\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contains(%s)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document_words' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
